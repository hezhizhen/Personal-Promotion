# Search in Depth

it is not enough to just use the `match` query; you need to understand your data and how you want to be able to search it

understand how each query contributes to the relevance `_score` help to tune your queries

## Structured Search

dates, times, numbers are all structured: they have a precise format that you can perform logical operations on

text can be structured

with structured search, the answer is always a yes or no; something either belongs in the set or not; there is no concept for `more similar`

## Full-Text Search

## Multifield Search

## Proximity Matching

## Partial Matching

## Controlling Relevance

structured data: dates, numbers, string enums, etc

full-text search engines have to not only find the matching documents, but also sort them by relevance

### Theory Behind Relevance Scoring

Lucene uses the `Boolean model` to find matching documents, and a formula `practical scoring function` to calculate relevance

the formula borrows concepts from `term frequency/inverse document frequency` and the `vector space model` but adds more-modern features like a coordination factor, field length normalization, and term or query clause boosting

`Boolean Model` simply applies the `AND`, `OR`, `NOT` conditions expressed in the query; it is simple and fast; used to exclude any document that cannot possibly match the query

not all documents will contain all the terms, and some terms are more important than others; the relevance score of the whole document depends on the `weight` of each query term that appears in the document

the weight is determined by 3 factors

TF: the more often, the higher the weight; calculated as follows `tf(t in d) = \sqrt{frequency}`

setting `index_options` to `docs` will disable term frequencies and term positions; exact-value `not_analyzed` string fields use this setting by default

IDF: the more often, the lower the weight; help zoom in on the most interesting documents; calculated as follow `idf(t) = 1+log(numDocs/(docFreq+1))`

Field-length norm: the shorter the field, the higher the weight; calculated as follow `norm(d) = 1/sqrt{numTerms}`

TF, IDF, FLN are calculated and store at index time; together they are used to calculate the weight of a single term in particular document

`vector space model` provides a way of comparing a multiterm query against a document; the output is a single score that represents how well the document matches the query; the model represents both the document and the query as vectors

each number in the vector is the weight of a term

TF/IDF is the default way of calculating term weights for the vector space model, but not the only way

### Lucene's Pratical Scoring Function

for multiterm queries: it takes the boolean model, TF/IDF and the vector space model and combines them in a single efficient package

`bool` query implements the boolean model

`practical scoring function`: teh formula used for scoring

`query normalization factor (queryNorm)`: an attempt to normalize a query so that the results from one query may be compared with the results of another

the factor is calculated at the beginning of the query; the actual calculation depends on the queries involved `queryNorm = 1/sqrt{sumOfSquaredWeights}`



### Query-Time Boosting

### Mannipulating Relevance with Query

### Structure

### Not Quite Not

### Ignoring TF/IDF

### function_score Query

`function_score` query: apply a function to each document that matches the main query in order to alter or completely replace the original query `_score`

some predefined functions out of the box

- `weight`                                 : appy a simple boost to each document without the boost being normalized (a weight of 2 results in 2*_score)
- `field_value_factor`                     : use the value of a field in the document to alter the `_score`
- `random_score`                           : use consistently random scoring to sort results differently for every user, while maintaining the same sort order for a single user
- **Decay functions** (linear, exp, gauss) : incorporate sliding-scale values like publish_date, geo_location, price into the `_score` to prefer recently published documents ......
- `script_score`                           : use a custom script to take complete control of the scoring logic

### Boosting Filtered Subsets

the full-text `_score` range usually falls somewhere between 0 and 10

