# 2 Search in Depth

it is not enough to just use the `match` query; you need to understand your data and how you want to be able to search it

understand how each query contributes to the relevance `_score` help to tune your queries

## 2.1 Structured Search

dates, times, numbers are all structured: they have a precise format that you can perform logical operations on

text can be structured

with structured search, the answer is always a yes or no; something either belongs in the set or not; there is no concept for `more similar`

## 2.2 Full-Text Search

*full-text* search: how to search within full-text fields in order to find the most relevant documents 

2 most important aspects of full-text search: 

* relevance: rank results; calculated by TF/IDF, proximity (to a geolocation), fuzzy similarity, or some other algorithm
* analysis: process of converting a block of text into distinct and normalized tokens in order to (1) create an inverted index and (2) query the inverted index
* in the territory of **queries** rather than **filters**

### 2.2.1 Term-Based Versus Full-Text

* all queries perform some sort of relevance calculation
* **NOT** all queries have an analysis phase
* specialized queries like the `bool` or `function_score` queries don't operate on text at all

textual queries can be broken down into 2 families: 

* Term-based queries
    * like `term` or `fuzzy` queries
    * low-level queries
    * have no analysis phase
    * operate on a single term
* Full-text queries
    * like `match` or `query_string` queries
    * high-level queries
    * understand the mapping of a field
        * if query a `date` field, they will treat the **query string** as a date
        * if query an exact value (`not_analyzed`) string field, they will treat the whole query string as a single term
        * if query a full-text (`analyzed`) field, they will pass the query string to produce the list of terms to be queried
    * once the query has assembled a list of terms, it executes the appropriate low-level query for each of these terms, and then combines their results to produce the final **relevance score** for each document

seldom need to use the term-based queries directly (they are used internally)

if want to use a query on an exact value `not_analyzed` field, think about:

1. do you really want a scoring query
2. if a non-scoring query might be better

**single-term queries** represent binary Y/N questions, and are better expressed as a **filter**

```elasticsearch
{
    "query": {
        "constant_score": {
            "filter": {
                "term": {
                    "gender": "female"
                }
            }
        }
    }
}
```

## 2.3 Multifield Search

## 2.4 Proximity Matching

## 2.5 Partial Matching

## 2.6 Controlling Relevance

structured data: dates, numbers, string enums, etc

full-text search engines have to not only find the matching documents, but also sort them by relevance

### Theory Behind Relevance Scoring

Lucene uses the `Boolean model` to find matching documents, and a formula `practical scoring function` to calculate relevance

the formula borrows concepts from `term frequency/inverse document frequency` and the `vector space model` but adds more-modern features like a coordination factor, field length normalization, and term or query clause boosting

`Boolean Model` simply applies the `AND`, `OR`, `NOT` conditions expressed in the query; it is simple and fast; used to exclude any document that cannot possibly match the query

not all documents will contain all the terms, and some terms are more important than others; the relevance score of the whole document depends on the `weight` of each query term that appears in the document

the weight is determined by 3 factors

TF: the more often, the higher the weight; calculated as follows `tf(t in d) = \sqrt{frequency}`

setting `index_options` to `docs` will disable term frequencies and term positions; exact-value `not_analyzed` string fields use this setting by default

IDF: the more often, the lower the weight; help zoom in on the most interesting documents; calculated as follow `idf(t) = 1+log(numDocs/(docFreq+1))`

Field-length norm: the shorter the field, the higher the weight; calculated as follow `norm(d) = 1/sqrt{numTerms}`

TF, IDF, FLN are calculated and store at index time; together they are used to calculate the weight of a single term in particular document

`vector space model` provides a way of comparing a multiterm query against a document; the output is a single score that represents how well the document matches the query; the model represents both the document and the query as vectors

each number in the vector is the weight of a term

TF/IDF is the default way of calculating term weights for the vector space model, but not the only way

### Lucene's Pratical Scoring Function

for multiterm queries: it takes the boolean model, TF/IDF and the vector space model and combines them in a single efficient package

`bool` query implements the boolean model

`practical scoring function`: teh formula used for scoring

`query normalization factor (queryNorm)`: an attempt to normalize a query so that the results from one query may be compared with the results of another

the factor is calculated at the beginning of the query; the actual calculation depends on the queries involved `queryNorm = 1/sqrt{sumOfSquaredWeights}`



### Query-Time Boosting

### Mannipulating Relevance with Query

### Structure

### Not Quite Not

### Ignoring TF/IDF

### function_score Query

`function_score` query: apply a function to each document that matches the main query in order to alter or completely replace the original query `_score`

some predefined functions out of the box

- `weight`                                 : appy a simple boost to each document without the boost being normalized (a weight of 2 results in 2*_score)
- `field_value_factor`                     : use the value of a field in the document to alter the `_score`
- `random_score`                           : use consistently random scoring to sort results differently for every user, while maintaining the same sort order for a single user
- **Decay functions** (linear, exp, gauss) : incorporate sliding-scale values like publish_date, geo_location, price into the `_score` to prefer recently published documents ......
- `script_score`                           : use a custom script to take complete control of the scoring logic

### Boosting Filtered Subsets

the full-text `_score` range usually falls somewhere between 0 and 10

